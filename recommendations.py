"""
GTFS Network Analysis Recommendations Generator

This script loads the CSV results generated by the 'network_analysis.py'
script and produces actionable recommendations based on predefined thresholds.

Recommendations Generated:
1.  Route Rationalization: Identifies pairs of routes with high overlap.
2.  Accessibility Improvement: Identifies stops with poor access to target hubs.
3.  Route Efficiency: Identifies routes with high circuity (inefficiency).
4.  Hub Improvement: Identifies critical transfer hubs (high betweenness).
5.  Mode Competition (Optional): Compares Public Transport (PT) travel times
    from analysis with estimated driving times from Google Directions API for
    a sample of OD pairs.

Setup:
1.  Ensure the analysis CSV files exist in `ANALYSIS_DIR`.
2.  Ensure GTFS files exist in `GTFS_DIR`.
3.  The script will attempt to create a 'recommendations' subdirector within
    `ANALYSIS_DIR` to save the output CSVs.
4.  (Optional) For Recommendation 5, set the `GOOGLE_API_KEY`, ensure the
    `googlemaps` library is installed (`pip install googlemaps`), and that
    the Directions API is enabled in your Google Cloud Project.
"""


import logging
import os
import time
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union

# Third-party libraries
try:
    import numpy as np
    import pandas as pd
except ImportError as e:
    print(f"Error: Missing essential library ({e}). Please install requirements.")
    print("pip install pandas numpy")
    exit(1)

# Optional Google Maps library, remove if not relevant
try:
    import googlemaps
    GOOGLEMAPS_AVAILABLE = True
except ImportError:
    logging.warning(
        "Optional 'googlemaps' library not installed. Google API comparison (Recommendation 5) "
        "will be skipped. Install with 'pip install googlemaps'"
    )
    GOOGLEMAPS_AVAILABLE = False


# --- Configuration Constants ---

# Directories
ANALYSIS_DIR: str = './network_analysis_output/'  # Input dir for analysis CSVs
GTFS_DIR: str = './gtfs/'                     # Input dir for base GTFS files
RECOMMENDATIONS_SUBDIR_NAME: str = 'recommendations' # Name of the output subdir
# Derived output directory path
RECOMMENDATIONS_DIR: str = os.path.join(ANALYSIS_DIR, RECOMMENDATIONS_SUBDIR_NAME)

# Thresholds for Recommendations
OVERLAP_THRESHOLD: float = 0.70  # Min overlap value for Rec 1 (e.g., 70%)
ACCESSIBILITY_TIME_MIN: int = 60 # Time threshold (min) column for Rec 2
MAX_REACHABLE_TARGETS: int = 1   # Max targets reached for poor access (Rec 2)
CIRCUITY_THRESHOLD: float = 2.0  # Min circuity index for inefficiency (Rec 3)

# Limits for Reporting Top N items
TOP_N_OVERLAP: int = 20      # Max overlapping pairs to list/save
TOP_N_POOR_ACCESS: int = 20  # Max poor access stops to list/save
TOP_N_CIRCUITY: int = 10     # Max inefficient routes to list/save
TOP_N_HUBS: int = 10         # Max critical hubs (betweenness) to list/save
TOP_N_COMPARISON: int = 10   # Max OD pairs for PT vs Driving comparison (Rec 5)

# --- Google API Configuration (Optiona) ---
GOOGLE_API_KEY: Optional[str] = "YOUR_API_KEY" # Set to None to disable
if GOOGLE_API_KEY == "YOUR_API_KEY":
    GOOGLE_API_KEY = None # Default to disabled if api key not changed

# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)


# --- Helper Functions ---

def ensure_dir_exists(dir_path: str) -> bool:
    """
    Creates a directory if it doesn't already exist.

    Args:
        dir_path: The path to the directory to create.

    Returns:
        True if the directory exists or was created successfully, False otherwise.
    """
    if not os.path.exists(dir_path):
        try:
            os.makedirs(dir_path)
            logging.info(f"Created directory: {dir_path}")
            return True
        except OSError as e:
            logging.error(f"Failed to create directory {dir_path}: {e}")
            return False
    return True

def safe_save_csv(df: pd.DataFrame, path: str, description: str) -> None:
    """
    Safely saves a DataFrame to CSV with logging. Creates directory if needed.

    Args:
        df: The pandas DataFrame to save.
        path: The full path to the output CSV file.
        description: A brief description of the data being saved (for logging).
    """
    if df is None or df.empty:
        logging.warning(f"Skipping save for {description}: DataFrame is empty or None.")
        return
    try:
        # Ensure the directory for the file exists
        file_dir = os.path.dirname(path)
        ensure_dir_exists(file_dir)
        # Save the DataFrame
        df.to_csv(path, index=False, encoding='utf-8') # Specify UTF-8 encoding
        logging.info(f"Successfully saved {description} to {path}")
    except Exception as e:
        logging.error(f"Failed to save {description} to {path}: {e}", exc_info=True)

def load_analysis_csv(
    file_path: str,
    required_cols: Optional[List[str]] = None,
    dtypes: Optional[Dict[str, Any]] = None,
    id_cols_to_str: Optional[List[str]] = None
    ) -> Optional[pd.DataFrame]:
    """
    Loads a CSV file, handling potential errors, checking columns, standardizing
    column names to lowercase, and converting specified ID columns to string type.

    Args:
        file_path: Full path to the CSV file.
        required_cols: List of required column names (case-insensitive check).
        dtypes: Dictionary specifying data types for columns during loading.
        id_cols_to_str: List of column names to explicitly convert to string type
                        after loading (handles cases like '123.0').

    Returns:
        A pandas DataFrame if loading is successful, otherwise None.
    """
    if not os.path.exists(file_path):
        logging.error(f"File not found: {file_path}")
        return None

    df: Optional[pd.DataFrame] = None
    encodings_to_try: List[str] = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']

    for enc in encodings_to_try:
        try:
            df = pd.read_csv(file_path, dtype=dtypes, encoding=enc, low_memory=False)
            logging.info(f"Successfully loaded {os.path.basename(file_path)} using encoding '{enc}'.")
            break # Exit loop on successful load
        except UnicodeDecodeError:
            logging.debug(f"Failed to load {os.path.basename(file_path)} with encoding '{enc}'. Trying next.")
            continue
        except Exception as e:
            logging.error(f"Error loading {file_path} with encoding '{enc}': {e}", exc_info=True)
            return None # Return None on other read errors

    if df is None:
        logging.error(f"Failed to load {file_path} after trying encodings: {encodings_to_try}")
        return None

    # Standardize column names to lowercase *before* checking required columns
    original_columns = df.columns.tolist()
    df.columns = df.columns.str.lower()
    df_cols_lower = df.columns.tolist()
    logging.debug(f"Original columns: {original_columns}")
    logging.debug(f"Standardized columns: {df_cols_lower}")

    # Check for required columns (case-insensitive via lowercase check)
    if required_cols:
        missing_cols = [col for col in required_cols if col.lower() not in df_cols_lower]
        if missing_cols:
            logging.error(f"File {os.path.basename(file_path)} is missing required columns (expected lowercase): {missing_cols}")
            return None

    # Ensure specified ID columns are string type, handling '.0' suffix if present
    if id_cols_to_str:
        for id_col in id_cols_to_str:
            if id_col.lower() in df.columns:
                col_name = id_col.lower()
                try:
                    # Convert potentially float-like strings or numbers to int then str
                    # Handle NaNs gracefully - convert to pd.NA before astype(str)
                    df[col_name] = df[col_name].astype(float).fillna(pd.NA).astype('Int64').astype(str)
                except (ValueError, TypeError):
                     # If conversion fails, fall back to simple string conversion
                     df[col_name] = df[col_name].astype(str)
                # Clean common suffixes like '.0' resulting from float conversion
                df[col_name] = df[col_name].str.replace(r'\.0$', '', regex=True)
                # Handle pandas' string representation of NA if needed
                df[col_name] = df[col_name].replace('<NA>', None) # Replace with None or keep as needed


    return df

def get_google_driving_time(
    origin_coords: Tuple[float, float],
    destination_coords: Tuple[float, float],
    api_key: str
    ) -> Tuple[Optional[float], Optional[float]]:
    """
    Gets estimated driving time (seconds) and distance (meters) from Google Directions API.

    Requires the 'googlemaps' library and a valid API key.

    Args:
        origin_coords: Tuple of (latitude, longitude) for the origin.
        destination_coords: Tuple of (latitude, longitude) for the destination.
        api_key: Your Google Cloud Platform API key.

    Returns:
        A tuple containing (duration_seconds, distance_meters). Returns (None, None)
        if the API call fails or no route is found.
    """
    if not GOOGLEMAPS_AVAILABLE:
        # Warning already logged at import time
        return None, None
    # API key check is done by the caller (generate_recommendations)

    gmaps = googlemaps.Client(key=api_key)
    origin_str = f"{origin_coords[0]},{origin_coords[1]}"
    dest_str = f"{destination_coords[0]},{destination_coords[1]}"
    logging.debug(f"Querying Google Directions API (Driving): {origin_str} -> {dest_str}")

    try:
        now = datetime.now()
        directions_result = gmaps.directions(
            origin=origin_str,
            destination=dest_str,
            mode="driving",
            departure_time=now,
            traffic_model="best_guess" # Request traffic-aware duration
        )

        if directions_result and isinstance(directions_result, list) and directions_result[0].get('legs'):
            leg = directions_result[0]['legs'][0]
            # Prioritize duration_in_traffic if available
            duration_data = leg.get('duration_in_traffic', leg.get('duration'))
            duration_sec = duration_data.get('value') if duration_data else None
            distance_m = leg.get('distance', {}).get('value')

            if duration_sec is not None:
                logging.debug(f"Google API Driving Result: Duration={duration_sec}s, Distance={distance_m}m")
                return float(duration_sec), float(distance_m) if distance_m is not None else None
            else:
                logging.warning("Could not extract duration from Google API driving result.")
                return None, float(distance_m) if distance_m is not None else None
        else:
            logging.info(f"No driving route found by Google API between {origin_str} and {dest_str}.")
            return None, None
    except googlemaps.exceptions.ApiError as e:
        logging.error(f"Google Directions API Error: {e}")
        return None, None
    except Exception as e:
        logging.error(f"Unexpected error calling Google Directions API: {e}", exc_info=True)
        return None, None

def format_timedelta_minutes(seconds: Optional[Union[float, int]]) -> str:
    """
    Formats a duration in seconds into a string like 'X min'.

    Handles None, NaN, or infinite inputs gracefully.

    Args:
        seconds: The duration in seconds.

    Returns:
        Formatted string (e.g., "15 min"), "N/A" for invalid/missing input,
        or "Infinite" for non-finite numbers.
    """
    if seconds is None or pd.isna(seconds):
        return "N/A"
    if not np.isfinite(seconds):
        return "Infinite"
    try:
        minutes = int(round(seconds / 60))
        return f"{minutes} min"
    except (ValueError, TypeError):
        return "Invalid"


# --- Recommendation Functions ---

def recommend_route_rationalization(
    overlap_df: pd.DataFrame,
    routes_df: Optional[pd.DataFrame],
    threshold: float = OVERLAP_THRESHOLD,
    top_n: int = TOP_N_OVERLAP
    ) -> Optional[pd.DataFrame]:
    """
    Identifies route pairs with overlap exceeding a threshold for potential rationalization.

    Args:
        overlap_df: DataFrame with route overlap analysis results.
        routes_df: DataFrame with route details (for names).
        threshold: Minimum overlap value (0.0 to 1.0) to consider.
        top_n: Maximum number of pairs to return and log.

    Returns:
        DataFrame of high-overlap route pairs with details, or None if none found.
    """
    logging.info(f"\n--- Generating Recommendation 1: Route Rationalization (Overlap >= {threshold:.0%}) ---")
    if 'overlap_value' not in overlap_df.columns:
         logging.error("Overlap DataFrame missing 'overlap_value' column.")
         return None

    high_overlap_pairs = overlap_df[overlap_df['overlap_value'] >= threshold].copy()
    high_overlap_pairs.sort_values(by='overlap_value', ascending=False, inplace=True)

    if high_overlap_pairs.empty:
        logging.info(f"No route pairs found with overlap >= {threshold:.0%}.")
        return None

    logging.info(f"Found {len(high_overlap_pairs):,} route pairs with overlap >= {threshold:.0%}.")
    logging.info(f"Top {min(top_n, len(high_overlap_pairs))} pairs recommended for review:")

    overlap_to_save = high_overlap_pairs.head(top_n).copy()

    # Merge route names if available
    if routes_df is not None:
        routes_info_cols = ['route_id']
        if 'route_short_name' in routes_df.columns: routes_info_cols.append('route_short_name')
        if 'route_long_name' in routes_df.columns: routes_info_cols.append('route_long_name')
        routes_info = routes_df[routes_info_cols]
        # Use suffixes to distinguish route A and B info
        overlap_to_save = pd.merge(overlap_to_save, routes_info, left_on='route_a', right_on='route_id', how='left', suffixes=('', '_a'))
        overlap_to_save = pd.merge(overlap_to_save, routes_info, left_on='route_b', right_on='route_id', how='left', suffixes=('', '_b'))
        # Rename columns for clarity
        overlap_to_save.rename(columns={
             'route_short_name': 'a_route_short_name', 'route_long_name': 'a_route_long_name',
             'route_short_name_b': 'b_route_short_name', 'route_long_name_b': 'b_route_long_name',
             'route_id_a': 'a_route_id', 'route_id_b': 'b_route_id'}, # Keep original IDs if needed
             inplace=True)
        # Drop redundant route_id cols from merge if needed
        if 'route_id' in overlap_to_save.columns and 'route_id_a' in overlap_to_save.columns:
             overlap_to_save.drop(columns=['route_id_a'], inplace=True, errors='ignore')
        if 'route_id' in overlap_to_save.columns and 'route_id_b' in overlap_to_save.columns:
             overlap_to_save.drop(columns=['route_id_b'], inplace=True, errors='ignore')


    # Define display columns safely
    display_cols = ['route_a', 'a_route_short_name', 'route_b', 'b_route_short_name', 'overlap_value', 'shared_stops']
    display_cols = [col for col in display_cols if col in overlap_to_save.columns] # Ensure they exist
    logging.info("\n" + overlap_to_save[display_cols].to_string(index=False, na_rep='N/A'))

    return overlap_to_save


def recommend_accessibility_improvements(
    accessibility_df: pd.DataFrame,
    stops_df: Optional[pd.DataFrame],
    time_min: int = ACCESSIBILITY_TIME_MIN,
    max_targets: int = MAX_REACHABLE_TARGETS,
    top_n: int = TOP_N_POOR_ACCESS
    ) -> Optional[pd.DataFrame]:
    """
    Identifies stops with poor accessibility (reaching few targets within a time limit).

    Args:
        accessibility_df: DataFrame with accessibility analysis results.
        stops_df: DataFrame with stop details (for names/coordinates).
        time_min: The time threshold column (in minutes) to check.
        max_targets: Maximum number of targets reachable to be considered poor access.
        top_n: Maximum number of stops to return and log.

    Returns:
        DataFrame of stops with poor access, including details, or None if none found.
    """
    logging.info(f"\n--- Generating Recommendation 2: Improve Accessibility (<= {max_targets} targets in {time_min} min) ---")
    access_col_name = f'reachable_targets_{time_min}min'

    if access_col_name not in accessibility_df.columns:
        logging.error(f"Required accessibility column '{access_col_name}' not found in input file.")
        return None

    # Filter stops meeting the poor access criteria
    # Ensure the column is numeric, coercing errors to NaN, then fill NaN with a value that won't meet the criteria (e.g., infinity or a large number)
    accessibility_df[access_col_name] = pd.to_numeric(accessibility_df[access_col_name], errors='coerce').fillna(float('inf'))
    poor_access_stops = accessibility_df[accessibility_df[access_col_name] <= max_targets].copy()
    poor_access_stops.sort_values(by=access_col_name, ascending=True, inplace=True)

    if poor_access_stops.empty:
        logging.info(f"No stops found reaching <= {max_targets} target(s) within {time_min} min.")
        return None

    logging.info(f"Found {len(poor_access_stops):,} stops reaching <= {max_targets} target(s) within {time_min} min.")
    logging.info(f"Top {min(top_n, len(poor_access_stops))} stops with poorest access recommended for investigation:")

    poor_access_to_save = poor_access_stops.head(top_n).copy()

    # Merge stop names and coordinates if available
    if stops_df is not None:
        poor_access_to_save = pd.merge(poor_access_to_save, stops_df[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']], on='stop_id', how='left')

    display_cols = ['stop_id', 'stop_name', access_col_name, 'stop_lat', 'stop_lon']
    display_cols = [col for col in display_cols if col in poor_access_to_save.columns]
    logging.info("\n" + poor_access_to_save[display_cols].to_string(index=False, na_rep='N/A'))

    return poor_access_to_save


def recommend_inefficient_routes(
    circuity_df: pd.DataFrame,
    routes_df: Optional[pd.DataFrame],
    threshold: float = CIRCUITY_THRESHOLD,
    top_n: int = TOP_N_CIRCUITY
    ) -> Optional[pd.DataFrame]:
    """
    Identifies routes with high circuity index, suggesting inefficiency.

    Args:
        circuity_df: DataFrame with route circuity analysis results.
        routes_df: DataFrame with route details (for names).
        threshold: Minimum circuity index to consider inefficient.
        top_n: Maximum number of routes to return and log.

    Returns:
        DataFrame of inefficient routes with details, or None if none found.
    """
    logging.info(f"\n--- Generating Recommendation 3: Address Route Inefficiencies (Circuity >= {threshold:.1f}) ---")
    if 'circuity_index' not in circuity_df.columns:
         logging.error("Circuity DataFrame missing 'circuity_index' column.")
         return None

    # Ensure circuity index is numeric, coerce errors, fill NaN
    circuity_df['circuity_index'] = pd.to_numeric(circuity_df['circuity_index'], errors='coerce').fillna(0.0)
    inefficient_routes = circuity_df[circuity_df['circuity_index'] >= threshold].copy()
    inefficient_routes.sort_values(by='circuity_index', ascending=False, inplace=True)

    if inefficient_routes.empty:
        logging.info(f"No routes found with circuity index >= {threshold:.1f}.")
        return None

    logging.info(f"Found {len(inefficient_routes):,} routes with circuity index >= {threshold:.1f}.")
    logging.info(f"Top {min(top_n, len(inefficient_routes))} most inefficient routes recommended for analysis:")

    inefficient_to_save = inefficient_routes.head(top_n).copy()

    # Merge route names if available
    if routes_df is not None:
        routes_info_cols = ['route_id']
        if 'route_short_name' in routes_df.columns: routes_info_cols.append('route_short_name')
        if 'route_long_name' in routes_df.columns: routes_info_cols.append('route_long_name')
        routes_info = routes_df[routes_info_cols]
        inefficient_to_save = pd.merge(inefficient_to_save, routes_info, on='route_id', how='left')

    # Format time columns for display if they exist
    if 'actual_time_seconds' in inefficient_to_save.columns:
        inefficient_to_save['actual_time'] = inefficient_to_save['actual_time_seconds'].apply(format_timedelta_minutes)
    if 'shortest_path_time_seconds' in inefficient_to_save.columns:
        inefficient_to_save['shortest_time'] = inefficient_to_save['shortest_path_time_seconds'].apply(format_timedelta_minutes)


    display_cols = ['route_id', 'route_short_name', 'route_long_name', 'circuity_index', 'actual_time', 'shortest_time']
    display_cols = [col for col in display_cols if col in inefficient_to_save.columns]
    logging.info("\n" + inefficient_to_save[display_cols].to_string(index=False, float_format='%.2f', na_rep='N/A'))

    return inefficient_to_save


def recommend_critical_hubs(
    betweenness_df: pd.DataFrame,
    stops_df: Optional[pd.DataFrame],
    top_n: int = TOP_N_HUBS
    ) -> Optional[pd.DataFrame]:
    """
    Identifies critical transfer hubs based on high betweenness centrality.

    Args:
        betweenness_df: DataFrame with betweenness centrality results.
        stops_df: DataFrame with stop details (for names/coordinates).
        top_n: Maximum number of hubs to return and log.

    Returns:
        DataFrame of critical hubs with details, or None if input is empty.
    """
    logging.info(f"\n--- Generating Recommendation 4: Focus on Critical Transfer Hubs (Top {top_n} by Betweenness) ---")
    if 'betweenness_centrality' not in betweenness_df.columns:
        logging.error("Betweenness DataFrame missing 'betweenness_centrality' column.")
        return None
    if betweenness_df.empty:
         logging.info("Betweenness centrality data is empty. Cannot identify hubs.")
         return None

    # Ensure centrality is numeric, sort, and take top N
    betweenness_df['betweenness_centrality'] = pd.to_numeric(betweenness_df['betweenness_centrality'], errors='coerce').fillna(0.0)
    top_hubs = betweenness_df.sort_values(by='betweenness_centrality', ascending=False).head(top_n).copy()

    if top_hubs.empty: # Should not happen if input df was not empty, but check anyway
        logging.info("No hubs identified after sorting (check data).")
        return None

    logging.info(f"Top {len(top_hubs)} critical hubs (by betweenness centrality) recommended for infrastructure/service review:")

    # Merge stop names and coordinates if available
    if stops_df is not None:
        top_hubs = pd.merge(top_hubs, stops_df[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']], on='stop_id', how='left')

    display_cols = ['stop_id', 'stop_name', 'betweenness_centrality', 'stop_lat', 'stop_lon']
    display_cols = [col for col in display_cols if col in top_hubs.columns]
    logging.info("\n" + top_hubs[display_cols].to_string(index=False, float_format='%.4f', na_rep='N/A'))

    return top_hubs


def compare_pt_vs_driving(
    transfers_df: pd.DataFrame,
    stops_df: pd.DataFrame,
    api_key: str,
    top_n: int = TOP_N_COMPARISON
    ) -> Optional[pd.DataFrame]:
    """
    Compares analyzed PT travel time with estimated Google Maps driving time for sample OD pairs.

    Args:
        transfers_df: DataFrame with transfer analysis results (including travel time).
        stops_df: DataFrame with stop details (must include lat/lon).
        api_key: Google Cloud Platform API key for Directions API.
        top_n: Maximum number of OD pairs (sorted by longest PT time) to compare.

    Returns:
        DataFrame comparing PT and driving times for the sample, or None if comparison fails.
    """
    logging.info(f"\n--- Generating Recommendation 5: Compare PT vs Driving Time (Top {top_n} Slowest PT OD Pairs) ---")

    if 'travel_time_seconds' not in transfers_df.columns:
         logging.error("Transfers DataFrame missing 'travel_time_seconds' column.")
         return None
    if not all(col in stops_df.columns for col in ['stop_id', 'stop_lat', 'stop_lon']):
         logging.error("Stops DataFrame missing 'stop_id', 'stop_lat', or 'stop_lon'.")
         return None

    # Filter valid PT times, sort by descending time, take top N
    transfers_df['travel_time_seconds'] = pd.to_numeric(transfers_df['travel_time_seconds'], errors='coerce')
    relevant_transfers = transfers_df[transfers_df['travel_time_seconds'].notna() & np.isfinite(transfers_df['travel_time_seconds'])].copy()
    relevant_transfers.sort_values(by='travel_time_seconds', ascending=False, inplace=True)
    od_pairs_to_compare = relevant_transfers.head(top_n)

    if od_pairs_to_compare.empty:
        logging.warning("No valid OD pairs with finite travel times found in transfers data for comparison.")
        return None

    logging.info(f"Processing {len(od_pairs_to_compare):,} OD pairs for PT vs Driving comparison")

    # Merge coordinates and names
    stops_info = stops_df[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']]
    try:
        od_merged = pd.merge(od_pairs_to_compare, stops_info.add_prefix('origin_'), left_on='origin', right_on='origin_stop_id', how='left')
        od_merged = pd.merge(od_merged, stops_info.add_prefix('dest_'), left_on='destination', right_on='dest_stop_id', how='left')
    except Exception as e:
         logging.error(f"Failed to merge stop information for OD pairs: {e}")
         return None

    pt_vs_driving_results: List[Dict[str, Any]] = []
    processed_count = 0

    # Iterate and call Google API
    for _, row in od_merged.iterrows():
        origin_id = row['origin']
        dest_id = row['destination']
        pt_time_sec = row['travel_time_seconds']
        origin_name = row.get('origin_stop_name', origin_id)
        dest_name = row.get('dest_stop_name', dest_id)

        # Check coordinates validity
        if pd.notna(row['origin_stop_lat']) and pd.notna(row['origin_stop_lon']) and pd.notna(row['dest_stop_lat']) and pd.notna(row['dest_stop_lon']):
            origin_coords = (row['origin_stop_lat'], row['origin_stop_lon'])
            dest_coords = (row['dest_stop_lat'], row['dest_stop_lon'])

            logging.info(f"\nComparing OD {processed_count+1}/{len(od_pairs_to_compare)}: {origin_name} ({origin_id}) -> {dest_name} ({dest_id})")
            logging.info(f"  > Public Transport Time (Analyzed): {format_timedelta_minutes(pt_time_sec)}")

            driving_time_sec, driving_dist_m = get_google_driving_time(origin_coords, dest_coords, api_key=api_key)

            result_row = {
                'origin_id': origin_id, 'origin_name': origin_name,
                'destination_id': dest_id, 'destination_name': dest_name,
                'pt_time_seconds': pt_time_sec,
                'driving_time_seconds': driving_time_sec,
                'driving_distance_meters': driving_dist_m,
                'time_difference_seconds': None,
                'pt_vs_driving_comparison': 'Driving time N/A'
            }

            if driving_time_sec is not None:
                time_diff = pt_time_sec - driving_time_sec
                result_row['time_difference_seconds'] = time_diff
                comparison_str = f"{format_timedelta_minutes(abs(time_diff))} {'PT Faster' if time_diff < 0 else 'Driving Faster'}"
                result_row['pt_vs_driving_comparison'] = comparison_str
                logging.info(f"  > Estimated Driving Time (Google):    {format_timedelta_minutes(driving_time_sec)}")
                logging.info(f"  > PT vs Driving Difference:         {comparison_str}")
            else:
                logging.info("  > Could not retrieve driving time from Google API.")

            pt_vs_driving_results.append(result_row)
            processed_count += 1
            time.sleep(0.1) # Small delay to avoid hitting API rate limits too quickly

        else:
            logging.warning(f"Skipping OD pair {origin_id} -> {dest_id} due to missing coordinates.")
            # Optionally add a row indicating skipped pair
            pt_vs_driving_results.append({
                'origin_id': origin_id, 'origin_name': origin_name,
                'destination_id': dest_id, 'destination_name': dest_name,
                'pt_time_seconds': pt_time_sec, 'driving_time_seconds': None,
                'driving_distance_meters': None, 'time_difference_seconds': None,
                'pt_vs_driving_comparison': 'Missing Coordinates'
            })
            processed_count += 1 # Still count as processed

    if not pt_vs_driving_results:
        logging.warning("No comparison results were generated.")
        return None

    comparison_df = pd.DataFrame(pt_vs_driving_results)
    # Add formatted time columns for readability
    comparison_df['pt_time_formatted'] = comparison_df['pt_time_seconds'].apply(format_timedelta_minutes)
    comparison_df['driving_time_formatted'] = comparison_df['driving_time_seconds'].apply(format_timedelta_minutes)

    return comparison_df


# --- Main Orchestration Function ---

def generate_recommendations():
    """Loads analysis data and generates recommendations."""
    main_start_time = time.time()
    logging.info("=" * 50)
    logging.info("Starting Recommendations Generation")
    logging.info(f"Analysis Input Directory: {os.path.abspath(ANALYSIS_DIR)}")
    logging.info(f"GTFS Input Directory:     {os.path.abspath(GTFS_DIR)}")
    logging.info(f"Output Directory:         {os.path.abspath(RECOMMENDATIONS_DIR)}")
    logging.info("=" * 50)

    if not ensure_dir_exists(RECOMMENDATIONS_DIR):
        logging.critical("Failed to create or access recommendations output directory. Exiting.")
        return

    # --- Load Base GTFS Data (Stops & Routes) ---
    logging.info("Loading base GTFS data (stops, routes)")
    stops_df = load_analysis_csv(
        os.path.join(GTFS_DIR, 'stops.txt'),
        required_cols=['stop_id', 'stop_name', 'stop_lat', 'stop_lon'],
        id_cols_to_str=['stop_id']
    )
    routes_df = load_analysis_csv(
        os.path.join(GTFS_DIR, 'routes.txt'),
        required_cols=['route_id'], 
        id_cols_to_str=['route_id']
    )
    if stops_df is None:
         logging.warning("Could not load stops.txt. Stop names/coordinates will be missing in recommendations.")
    if routes_df is None:
         logging.warning("Could not load routes.txt. Route names will be missing in recommendations.")


    # --- Load Analysis Results ---
    logging.info("\n--- Loading Analysis Results ---")
    id_str_cols = ['stop_id', 'route_id', 'route_a', 'route_b', 'origin', 'destination'] # Common ID cols

    overlap_df = load_analysis_csv(
        os.path.join(ANALYSIS_DIR, 'route_overlap.csv'),
        required_cols=['route_a', 'route_b', 'overlap_value'],
        id_cols_to_str=id_str_cols
    )
    accessibility_df = load_analysis_csv(
        os.path.join(ANALYSIS_DIR, 'accessibility_to_targets.csv'),
        # Column name depends on threshold used in analysis script
        required_cols=[f'reachable_targets_{ACCESSIBILITY_TIME_MIN}min'],
        id_cols_to_str=id_str_cols
    )
    circuity_df = load_analysis_csv(
        os.path.join(ANALYSIS_DIR, 'route_circuity.csv'),
        required_cols=['route_id', 'circuity_index'],
        id_cols_to_str=id_str_cols
    )
    betweenness_df = load_analysis_csv(
        os.path.join(ANALYSIS_DIR, 'betweenness_centrality.csv'),
        required_cols=['stop_id', 'betweenness_centrality'],
        id_cols_to_str=id_str_cols
    )
    transfers_df = load_analysis_csv( # Needed for Rec 5
        os.path.join(ANALYSIS_DIR, 'transfer_analysis_sample.csv'),
        required_cols=['origin', 'destination', 'travel_time_seconds'],
        id_cols_to_str=id_str_cols
    )


    # --- Generate and Save Recommendations ---

    # Rec 1: Overlap
    if overlap_df is not None:
        rec1_df = recommend_route_rationalization(overlap_df, routes_df)
        safe_save_csv(rec1_df, os.path.join(RECOMMENDATIONS_DIR, 'rec1_high_overlap_routes.csv'), "Rec 1: High Overlap Routes")
    else:
        logging.warning("Skipping Recommendation 1: Overlap data failed to load.")

    # Rec 2: Accessibility
    if accessibility_df is not None:
        rec2_df = recommend_accessibility_improvements(accessibility_df, stops_df)
        safe_save_csv(rec2_df, os.path.join(RECOMMENDATIONS_DIR, 'rec2_poor_access_stops.csv'), "Rec 2: Poor Access Stops")
    else:
        logging.warning(f"Skipping Recommendation 2: Accessibility data (for {ACCESSIBILITY_TIME_MIN} min) failed to load.")

    # Rec 3: Circuity
    if circuity_df is not None:
        rec3_df = recommend_inefficient_routes(circuity_df, routes_df)
        safe_save_csv(rec3_df, os.path.join(RECOMMENDATIONS_DIR, 'rec3_inefficient_routes.csv'), "Rec 3: Inefficient Routes")
    else:
        logging.warning("Skipping Recommendation 3: Circuity data failed to load.")

    # Rec 4: Betweenness Hubs
    if betweenness_df is not None:
        rec4_df = recommend_critical_hubs(betweenness_df, stops_df)
        safe_save_csv(rec4_df, os.path.join(RECOMMENDATIONS_DIR, 'rec4_critical_hubs.csv'), "Rec 4: Critical Hubs")
    else:
        logging.warning("Skipping Recommendation 4: Betweenness data failed to load.")

    # Rec 5: PT vs Driving Comparison (Optional)
    if GOOGLEMAPS_AVAILABLE and GOOGLE_API_KEY is not None and transfers_df is not None and stops_df is not None:
        rec5_df = compare_pt_vs_driving(transfers_df, stops_df, GOOGLE_API_KEY)
        safe_save_csv(rec5_df, os.path.join(RECOMMENDATIONS_DIR, 'rec5_pt_vs_driving_comparison.csv'), "Rec 5: PT vs Driving Comparison")
    elif not GOOGLEMAPS_AVAILABLE:
        logging.info("Skipping Recommendation 5: googlemaps library not available.")
    elif GOOGLE_API_KEY is None:
        logging.info("Skipping Recommendation 5: Google API Key not configured.")
    elif transfers_df is None:
         logging.warning("Skipping Recommendation 5: Transfers data failed to load.")
    elif stops_df is None:
         logging.warning("Skipping Recommendation 5: Stops data failed to load (needed for coordinates).")


    # --- Script Finish ---
    total_runtime = time.time() - main_start_time
    logging.info("=" * 50)
    logging.info(f"Recommendations Generation Script Finished")
    logging.info(f"Total execution time: {total_runtime:.2f} seconds ({format_timedelta_minutes(total_runtime)})")
    logging.info(f"Recommendation files saved in: {os.path.abspath(RECOMMENDATIONS_DIR)}")
    logging.info("=" * 50)


if __name__ == "__main__":
    generate_recommendations()