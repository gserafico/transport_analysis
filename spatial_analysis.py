# -*- coding: utf-8 -*-
"""
GTFS Network Analysis Results Visualization

This script loads the CSV and JSON results generated by the
'gtfs_network_analysis.py' script and creates various non-spatial and
spatial visualizations to aid in interpretation.

Visualizations Generated:
- Non-Spatial:
    - Histograms of centrality measures (Betweenness, Degree, Closeness)
    - Histogram of accessibility scores
    - Histogram of route circuity values
    - Histogram of transfer counts (from sample OD pairs)
    - Bar plots of top N stops/routes for centrality, circuity, etc.
- Spatial (Optional - requires GeoPandas, Shapely, Contextily):
    - Map of stop locations colored by accessibility scores.
    - Map of stop locations colored by betweenness centrality (log scale).
    - Map of stop locations colored by degree centrality.
    - Map plotting the geometry of the Top N most circuitous routes.

Setup:
1.  Ensure the analysis output files exist in `ANALYSIS_DIR`.
2.  Ensure GTFS files (stops.txt, routes.txt) exist in `GTFS_DIR`.
3.  The script will attempt to create the directory specified by `PLOT_DIR`.
4.  (Optional) For spatial plots, install the necessary libraries:
    `pip install geopandas shapely contextily` or
    `conda install geopandas contextily -c conda-forge`
"""

import json
import logging
import os
import time
from typing import Any, Dict, List, Optional, Tuple, Union

# Standard Imports
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

# Optional Spatial Libraries
try:
    import contextily as ctx
    import geopandas as gpd
    from shapely.geometry import LineString, Point
    GEOPANDAS_AVAILABLE = True
    CONTEXTILY_AVAILABLE = True
except ImportError:
    logging.warning(
        "Optional libraries GeoPandas, Shapely or Contextily not found. "
        "Spatial plotting will be skipped."
    )
    logging.warning(
        "Install with 'pip install geopandas shapely contextily' or "
        "'conda install geopandas contextily -c conda-forge'"
    )
    GEOPANDAS_AVAILABLE = False
    CONTEXTILY_AVAILABLE = False
    # Define dummy types if libraries are missing to avoid NameErrors
    class Point: pass
    class LineString: pass
    # Use Type Alias for GeoDataFrame for type hinting robustness
    GeoDataFrame = Any # type: ignore


# --- Configuration Constants ---

# Directories
ANALYSIS_DIR: str = './network_analysis_output/' # Input dir for analysis CSVs/JSON
PLOT_DIR: str = './analysis_plots/'           # Output dir for plots
GTFS_DIR: str = './gtfs/'                     # Input dir for base GTFS files

# File Paths (relative to ANALYSIS_DIR or GTFS_DIR)
STOPS_FILE_PATH: str = os.path.join(GTFS_DIR, 'stops.txt')
ROUTES_FILE_PATH: str = os.path.join(GTFS_DIR, 'routes.txt')
ROUTE_SEQUENCES_PATH: str = os.path.join(ANALYSIS_DIR, 'route_sequences.json')

# Plotting Parameters
TOP_N: int = 20              # Default num items for top N bar plots
TOP_N_CIRCUITY_PLOT: int = 10 # Num top circuitous routes to plot spatially
DEFAULT_FIGSIZE: Tuple[int, int] = (10, 6)
SPATIAL_FIGSIZE: Tuple[int, int] = (12, 12)
SPATIAL_ROUTE_FIGSIZE: Tuple[int, int] = (14, 12) # Wider for legend
PLOT_DPI: int = 300 # Resolution for saved plots
TITLE_BBOX_PROPS: Dict[str, Any] = dict(
    boxstyle='square,pad=0.3', # Box style (square with padding)
    fc='white',               # Face color (white)
    ec='none',                # Edge color (no edge)
    alpha=0.9                 # Transparency (0=transparent, 1=opaque)
)

# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s [%(funcName)s] %(message)s', # Added funcName
    datefmt='%Y-%m-%d %H:%M:%S'
)

# --- Helper Functions ---

def ensure_dir_exists(dir_path: str) -> bool:
    """Creates a directory if it doesn't already exist."""
    if not os.path.exists(dir_path):
        try:
            os.makedirs(dir_path)
            logging.info(f"Created directory: {dir_path}")
            return True
        except OSError as e:
            logging.error(f"Failed to create directory {dir_path}: {e}")
            return False
    return True

def safe_save_plot(figure: plt.Figure, path: str, description: str) -> None:
    """Safely saves a matplotlib figure to a file with logging."""
    if figure is None:
        logging.warning(f"Skipping save for {description}: Figure is None.")
        return
    try:
        file_dir = os.path.dirname(path)
        ensure_dir_exists(file_dir)
        figure.savefig(path, dpi=PLOT_DPI, bbox_inches='tight')
        logging.info(f"Successfully saved {description} plot to {path}")
    except Exception as e:
        logging.error(f"Failed to save {description} plot to {path}: {e}", exc_info=True)
    finally:
        # Ensure plot is closed to free memory
        plt.close(figure)

def load_analysis_csv(
    filename: str,
    analysis_dir: str = ANALYSIS_DIR,
    required_cols: Optional[List[str]] = None,
    dtypes: Optional[Dict[str, Any]] = None,
    id_cols_to_str: Optional[List[str]] = None
    ) -> Optional[pd.DataFrame]:
    """
    Loads a CSV file, handles errors, checks columns, standardizes names,
    and converts specified ID columns to string type.

    Args:
        filename: Name of the CSV file.
        analysis_dir: Directory containing the analysis files.
        required_cols: List of required column names (case-insensitive check).
        dtypes: Dictionary specifying data types for columns during loading.
        id_cols_to_str: List of column names to explicitly convert to string type.

    Returns:
        A pandas DataFrame if loading is successful, otherwise None.
    """
    file_path = os.path.join(analysis_dir, filename)
    if not os.path.exists(file_path):
        logging.error(f"File not found: {file_path}")
        return None

    df: Optional[pd.DataFrame] = None
    encodings_to_try: List[str] = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']

    for enc in encodings_to_try:
        try:
            read_dtypes = dtypes if dtypes else {}
            df = pd.read_csv(file_path, dtype=read_dtypes, encoding=enc, low_memory=False)
            logging.info(f"Successfully loaded {filename} using encoding '{enc}'.")
            break
        except UnicodeDecodeError:
            logging.debug(f"Failed loading {filename} with encoding '{enc}'.")
            continue
        except Exception as e:
            logging.error(f"Error loading {file_path} with encoding '{enc}': {e}", exc_info=True)
            return None

    if df is None:
        logging.error(f"Failed to load {filename} after trying encodings: {encodings_to_try}")
        return None

    # Standardize column names to lowercase
    df.columns = df.columns.str.lower()
    df_cols_lower = df.columns.tolist()

    # Check for required columns
    if required_cols:
        missing_cols = [col.lower() for col in required_cols if col.lower() not in df_cols_lower]
        if missing_cols:
            logging.error(f"File {filename} missing required columns: {missing_cols}")
            return None

    # Ensure specified ID columns are string type (simplified approach)
    if id_cols_to_str:
        for id_col in id_cols_to_str:
            col_name = id_col.lower()
            if col_name in df.columns:
                try:
                    # Convert to string, filling potential NaNs with empty string first
                    df[col_name] = df[col_name].fillna('').astype(str)
                    # Optional: remove '.0' if IDs might have been loaded as float initially
                    df[col_name] = df[col_name].str.replace(r'\.0$', '', regex=True)
                except Exception as e:
                    # Log warning but continue; basic string conversion might still work
                    logging.warning(f"Error during specific string conversion for column '{col_name}' in {filename}: {e}. Column might not be ideal string type.")
                    # Attempt basic conversion as fallback
                    if not pd.api.types.is_string_dtype(df[col_name]):
                         df[col_name] = df[col_name].astype(str)


    return df

# --- Non-Spatial Plotting Functions ---

def plot_histogram(
    df: Optional[pd.DataFrame], column: str, title: str, xlabel: str, filename: str, bins: int = 30
    ) -> None:
    """Generates and saves a histogram for a given DataFrame column."""
    if df is None or column not in df.columns:
        logging.warning(f"Plotting skipped for '{title}': DataFrame is None or column '{column}' not found.")
        return

    numeric_col = pd.to_numeric(df[column], errors='coerce').dropna()
    if numeric_col.empty:
        logging.warning(f"Plotting skipped for '{title}': No valid numeric data in column '{column}'.")
        return

    fig, ax = plt.subplots(figsize=DEFAULT_FIGSIZE)
    sns.histplot(numeric_col, bins=bins, kde=True, ax=ax)
    ax.set_title(title,bbox=TITLE_BBOX_PROPS)
    ax.set_xlabel(xlabel)
    ax.set_ylabel("Frequency")
    plt.tight_layout()
    safe_save_plot(fig, os.path.join(PLOT_DIR, filename), f"Histogram: {title}")

def plot_top_n_bar(
    df: Optional[pd.DataFrame], value_col: str, label_col: str, title: str, xlabel: str, ylabel: str,
    filename: str, top_n: int = TOP_N, merge_df: Optional[pd.DataFrame] = None,
    merge_on: str = 'stop_id', merge_label_col: Optional[str] = None
    ) -> None:
    """Generates and saves a horizontal bar plot for Top N items, merging labels if possible."""
    if df is None or value_col not in df.columns or label_col not in df.columns:
        logging.warning(f"Plotting skipped for '{title}': DataFrame is None or required columns missing.")
        return

    df_plot = df.copy()
    df_plot[value_col] = pd.to_numeric(df_plot[value_col], errors='coerce')
    df_plot.dropna(subset=[value_col], inplace=True)
    if df_plot.empty:
        logging.warning(f"Plotting skipped for '{title}': No valid numeric data in '{value_col}'.")
        return

    # Prepare base display label (ensure it's string)
    df_plot['display_label'] = df_plot[label_col].fillna('').astype(str)
    display_label_col = 'display_label' # Use this column for plotting

    # Attempt to merge more descriptive labels
    if merge_df is not None and merge_label_col and merge_on in df_plot.columns:
        merge_label_col_lower = merge_label_col.lower() # Use lowercase for lookup
        if merge_label_col_lower in merge_df.columns:
            try:
                # Prepare merge keys (ensure string type)
                df_plot[merge_on] = df_plot[merge_on].astype(str)
                merge_df[merge_on] = merge_df[merge_on].astype(str) # Ensure merge_df key is also str

                # Perform merge
                df_plot_merged = pd.merge(
                    df_plot,
                    merge_df[[merge_on, merge_label_col_lower]],
                    on=merge_on,
                    how='left'
                )
                # Use merged label if it's not null/empty, otherwise keep original base label
                merged_labels = df_plot_merged[merge_label_col_lower].fillna('')
                df_plot_merged['display_label'] = np.where(
                    merged_labels != '',
                    merged_labels, # Use the merged name
                    df_plot_merged['display_label'] # Keep original ID if merge failed/empty
                )
                df_plot = df_plot_merged # Use the merged dataframe

            except Exception as e:
                logging.warning(f"Could not merge labels for plot '{title}' using column '{merge_label_col}': {e}. Using original '{label_col}'.")
        else:
             logging.warning(f"Merge label column '{merge_label_col}' (lowercase: '{merge_label_col_lower}') not found in merge_df for plot '{title}'.")

    top_items = df_plot.nlargest(top_n, value_col)
    if top_items.empty:
        logging.warning(f"Plotting skipped for '{title}': No items found after nlargest.")
        return

    fig, ax = plt.subplots(figsize=(12, max(6, len(top_items) * 0.4))) # Adjust height
    sns.barplot(data=top_items, y=display_label_col, x=value_col, palette='viridis', orient='h', ax=ax)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    plt.figtext(0.5, 0.96, title, ha='center', va='top', fontsize=14,bbox=TITLE_BBOX_PROPS)
    plt.tight_layout()
    safe_save_plot(fig, os.path.join(PLOT_DIR, filename), f"Bar Plot: {title}")


# --- Spatial Plotting Functions ---

def plot_spatial_distribution(
    geo_df: Optional['GeoDataFrame'], value_col: str, title: str, label: str, filename: str,
    cmap: str = 'viridis', log_scale: bool = False, markersize: int = 15
    ) -> None:
    """Plots stops spatially, colored by a specified value."""
    if geo_df is None or not isinstance(geo_df, gpd.GeoDataFrame) or geo_df.empty:
         logging.warning(f"Spatial plotting skipped for '{title}': Invalid or empty GeoDataFrame.")
         return
    if value_col not in geo_df.columns:
        logging.warning(f"Spatial plotting skipped for '{title}': Column '{value_col}' missing.")
        return

    plot_col = value_col
    plot_label = label
    geo_plot = geo_df.copy()

    # Ensure value column is numeric and finite
    geo_plot[value_col] = pd.to_numeric(geo_plot[value_col], errors='coerce')
    geo_plot.replace([np.inf, -np.inf], np.nan, inplace=True) # Replace infinities
    geo_plot.dropna(subset=[value_col, geo_plot.geometry.name], inplace=True) # Drop NaN values and invalid geometries

    if geo_plot.empty:
        logging.warning(f"Spatial plotting skipped for '{title}': No valid data/geometry after cleaning '{value_col}'.")
        return

    # Apply log scale if requested
    if log_scale:
        # Use log1p for robustness against zeros
        geo_plot[f'log_{value_col}'] = np.log1p(geo_plot[value_col])
        plot_col = f'log_{value_col}'
        plot_label = f"Log(1 + {label})"

    fig, ax = plt.subplots(1, 1, figsize=SPATIAL_FIGSIZE)
    try:
        geo_plot.plot(column=plot_col, ax=ax, legend=False, cmap=cmap, markersize=markersize, alpha=0.7)
    except Exception as plot_err:
        logging.error(f"Failed during geo_plot.plot for '{title}': {plot_err}")
        plt.close(fig)
        return

    # Add basemap safely
    if CONTEXTILY_AVAILABLE:
        try:
            ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron) # Use configured CRS (should be 3857)
        except Exception as e:
            logging.warning(f"Failed to add basemap for '{title}': {e}")

    ax.set_axis_off()

    # Add colorbar safely
    try:
        vmin = geo_plot[plot_col].min()
        vmax = geo_plot[plot_col].max()
        if pd.notna(vmin) and pd.notna(vmax) and vmin != vmax:
            norm = plt.Normalize(vmin=vmin, vmax=vmax)
            sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
            sm.set_array([]) # Avoid UserWarning
            cbar = fig.colorbar(sm, ax=ax, shrink=0.6, aspect=20, pad=0.02)
            cbar.set_label(plot_label, size='small')
            cbar.ax.tick_params(labelsize='small')
        elif vmin == vmax:
             logging.warning(f"Colorbar skipped for '{title}': All values are identical ({vmin}).")
        else:
             logging.warning(f"Colorbar skipped for '{title}': Invalid min/max values ({vmin}, {vmax}).")

    except Exception as e:
         logging.warning(f"Could not add colorbar for '{title}': {e}")

    plt.figtext(0.5, 0.01, title, ha='center', va='bottom', fontsize=10, bbox=TITLE_BBOX_PROPS)
    plt.tight_layout()
    safe_save_plot(fig, os.path.join(PLOT_DIR, filename), f"Spatial Plot: {title}")

def plot_circuitous_routes_map(
    top_circuity_routes: pd.DataFrame, # Includes 'route_id', 'display_name'
    route_sequences: Dict[str, List[str]],
    stops_geo: 'GeoDataFrame', # Must be projected (e.g., EPSG:3857) and indexed by stop_id
    filename: str,
    top_n_plot: int
    ) -> None:
    """Plots the geometry of the top N most circuitous routes on a map."""
    if top_circuity_routes.empty:
         logging.warning("No top circuitous routes data provided. Skipping map.")
         return
    if not route_sequences:
         logging.warning("Route sequences data not available. Skipping map.")
         return
    if stops_geo is None or not isinstance(stops_geo, gpd.GeoDataFrame) or stops_geo.empty:
        logging.warning("Valid projected stops GeoDataFrame not available. Skipping map.")
        return
    if not stops_geo.index.name == 'stop_id':
        logging.error("Stops GeoDataFrame must be indexed by 'stop_id' for route plotting.")
        return

    logging.info(f"Plotting map of Top {min(top_n_plot, len(top_circuity_routes))} most circuitous routes")

    route_geometries: List[LineString] = []
    route_display_names_for_legend: List[str] = []
    stops_geometry_lookup = stops_geo['geometry'] # Series indexed by stop_id

    # Create LineString geometry for each route
    routes_processed_count = 0
    for _, route_info in top_circuity_routes.iterrows():
        if routes_processed_count >= top_n_plot: break # Limit to requested number

        route_id = str(route_info['route_id'])
        display_name = str(route_info.get('display_name', route_id))
        stop_ids_sequence = route_sequences.get(route_id)

        if not stop_ids_sequence or len(stop_ids_sequence) < 2:
            logging.debug(f"Skipping route {route_id}: No valid sequence.")
            continue

        # Get coordinates for stops in sequence, handling missing stops
        coords_list: List[Point] = []
        for stop_id_str in map(str, stop_ids_sequence): # Ensure stop_id is string
            geom = stops_geometry_lookup.get(stop_id_str)
            if geom is not None:
                 coords_list.append(geom)
            else:
                 logging.debug(f"Stop ID '{stop_id_str}' from sequence of route '{route_id}' not found in stops_geo index.")

        if len(coords_list) >= 2:
            # Use coordinates directly from the (projected) geometry objects
            coord_tuples = [(geom.x, geom.y) for geom in coords_list]
            try:
                 line = LineString(coord_tuples)
                 route_geometries.append(line)
                 route_display_names_for_legend.append(display_name)
                 routes_processed_count += 1
            except Exception as line_err:
                 logging.warning(f"Could not create LineString for route {route_id}: {line_err}")
        else:
            logging.debug(f"Skipping route {route_id}: Not enough valid coordinates ({len(coords_list)}) found.")


    if not route_geometries:
        logging.warning("Could not generate geometries for any top circuitous routes.")
        return

    routes_gdf = gpd.GeoDataFrame(
        {'display_name': route_display_names_for_legend},
        geometry=route_geometries,
        crs=stops_geo.crs # Inherit CRS from stops_geo
    )
    logging.info(f"Generated geometries for {len(routes_gdf)} circuitous routes.")

    # --- Plotting ---
    fig, ax = plt.subplots(1, 1, figsize=SPATIAL_ROUTE_FIGSIZE)
    num_routes_plotted = len(routes_gdf)
    cmap = plt.get_cmap('tab20', max(20, num_routes_plotted))

    for i, row in routes_gdf.iterrows():
        label_name = row['display_name'][:30] + ('...' if len(row['display_name']) > 30 else '')
        gpd.GeoSeries([row.geometry], crs=routes_gdf.crs).plot(
            ax=ax, color=cmap(i % cmap.N), linewidth=1.5, alpha=0.8, label=label_name
        )

    if CONTEXTILY_AVAILABLE:
        try: ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)
        except Exception as e: logging.warning(f"Failed to add basemap: {e}")

    ax.set_axis_off()
    ax.legend(title="Route", fontsize='x-small', loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, ncol=1)

    title_text = f"Top {num_routes_plotted} Most Circuitous Routes"
    plt.figtext(0.5, 0.01, title_text, ha='center', va='bottom', fontsize=10,bbox=TITLE_BBOX_PROPS)
    plt.tight_layout() 
    safe_save_plot(fig, os.path.join(PLOT_DIR, filename), f"Spatial Plot: {title_text}")


# --- Main Orchestration Function ---

def run_visualization():
    """Loads analysis results and generates visualizations."""
    main_start_time = time.time()
    logging.info("=" * 50)
    logging.info("Starting Analysis Results Visualization Script")
    logging.info(f"Analysis Input Directory: {os.path.abspath(ANALYSIS_DIR)}")
    logging.info(f"Plot Output Directory:    {os.path.abspath(PLOT_DIR)}")
    logging.info("=" * 50)

    if not ensure_dir_exists(PLOT_DIR):
        logging.critical("Failed to create/access plot output directory. Exiting.")
        return

    # --- Load Base GTFS Data ---
    logging.info("Loading base GTFS data (stops, routes)")
    stops_df = load_analysis_csv(
        os.path.basename(STOPS_FILE_PATH), analysis_dir=GTFS_DIR,
        required_cols=['stop_id', 'stop_name', 'stop_lat', 'stop_lon'],
        dtypes={'stop_lat': float, 'stop_lon': float}, id_cols_to_str=['stop_id']
    )
    routes_df = load_analysis_csv(
        os.path.basename(ROUTES_FILE_PATH), analysis_dir=GTFS_DIR,
        required_cols=['route_id'], id_cols_to_str=['route_id']
    )

    # --- Load Analysis Results ---
    logging.info("Loading analysis results CSVs")
    id_str_cols = ['stop_id', 'route_id', 'route_a', 'route_b', 'origin', 'destination']
    connectivity_df = load_analysis_csv('connectivity_metrics.csv')
    degree_df = load_analysis_csv('total_degree.csv', id_cols_to_str=id_str_cols)
    closeness_df = load_analysis_csv('closeness_centrality.csv', id_cols_to_str=id_str_cols)
    betweenness_df = load_analysis_csv('betweenness_centrality.csv', id_cols_to_str=id_str_cols)
    accessibility_df = load_analysis_csv('accessibility_to_targets.csv', id_cols_to_str=id_str_cols)
    overlap_df = load_analysis_csv('route_overlap.csv', id_cols_to_str=id_str_cols)
    circuity_df = load_analysis_csv('route_circuity.csv', id_cols_to_str=id_str_cols)
    transfers_df = load_analysis_csv('transfer_analysis_sample.csv', id_cols_to_str=id_str_cols)
    resilience_summary_df = load_analysis_csv('resilience_analysis_summary.csv')

    # --- Load Route Sequences ---
    route_sequences: Optional[Dict[str, List[str]]] = None
    if os.path.exists(ROUTE_SEQUENCES_PATH):
        try:
            with open(ROUTE_SEQUENCES_PATH, 'r', encoding='utf-8') as f:
                route_sequences = json.load(f)
            logging.info(f"Loaded route sequences from {ROUTE_SEQUENCES_PATH}")
        except Exception as e: logging.error(f"Error loading route sequences JSON: {e}")
    else: logging.warning(f"Route sequences file not found: {ROUTE_SEQUENCES_PATH}")

    # --- Generate Non-Spatial Visualizations ---
    logging.info("\n--- Generating Non-Spatial Visualizations ---")
    plot_histogram(betweenness_df, 'betweenness_centrality', 'Distribution of Betweenness Centrality', 'Betweenness Centrality', 'hist_betweenness.png')
    plot_histogram(degree_df, 'total_degree', 'Distribution of Stop Degree', 'Total Degree', 'hist_degree.png')
    plot_histogram(closeness_df, 'closeness_centrality', 'Distribution of Closeness Centrality', 'Closeness Centrality', 'hist_closeness.png')
    plot_histogram(circuity_df, 'circuity_index', 'Distribution of Route Circuity', 'Circuity Index', 'hist_circuity.png')
    plot_histogram(transfers_df, 'num_transfers', 'Distribution of Transfers per Trip (Sample)', 'Number of Transfers', 'hist_transfers.png')

    if accessibility_df is not None:
        access_cols = [col for col in accessibility_df.columns if 'reachable_targets_' in col]
        if access_cols:
             try:
                  access_cols_sorted = sorted(access_cols, key=lambda x: int(x.split('_')[-1].replace('min','')))
                  longest_threshold_col = access_cols_sorted[-1]
                  time_val = longest_threshold_col.split('_')[-1]
                  plot_histogram(accessibility_df, longest_threshold_col, f'Distribution of Reachable Targets ({time_val})', f'Number of Reachable Targets ({time_val})', f'hist_accessibility_{time_val}.png')
             except (ValueError, IndexError) as e: logging.warning(f"Could not parse time value from accessibility columns: {access_cols}. Error: {e}")
        else: logging.warning("No accessibility columns for histogram.")

    plot_top_n_bar(betweenness_df, 'betweenness_centrality', 'stop_id', f'Top {TOP_N} Stops by Betweenness', 'Betweenness Centrality', 'Stop', 'bar_top_betweenness.png', merge_df=stops_df, merge_label_col='stop_name')
    plot_top_n_bar(degree_df, 'total_degree', 'stop_id', f'Top {TOP_N} Stops by Degree', 'Total Degree', 'Stop', 'bar_top_degree.png', merge_df=stops_df, merge_label_col='stop_name')
    # Try merging short name, then long name for circuity bar plot labels
    circuity_label_col = 'route_short_name' if routes_df is not None and 'route_short_name' in routes_df.columns else 'route_long_name'
    plot_top_n_bar(circuity_df, 'circuity_index', 'route_id', f'Top {TOP_N} Routes by Circuity', 'Circuity Index', 'Route', 'bar_top_circuity.png', merge_df=routes_df, merge_label_col=circuity_label_col)

    if resilience_summary_df is not None and 'scenario' in resilience_summary_df.columns and 'scc_change_pct' in resilience_summary_df.columns:
         resilience_summary_df['scc_change_pct'] = pd.to_numeric(resilience_summary_df['scc_change_pct'], errors='coerce')
         resilience_summary_df.dropna(subset=['scc_change_pct'], inplace=True)
         if not resilience_summary_df.empty:
              fig, ax = plt.subplots(figsize=(10, 5))
              sns.barplot(data=resilience_summary_df, x='scenario', y='scc_change_pct', palette='coolwarm', ax=ax)
              ax.set_ylabel('Largest SCC Size Change (%)')
              ax.set_xlabel('Resilience Scenario')
              ax.set_title('Impact of Node Removal on Largest SCC Size',bbox=TITLE_BBOX_PROPS)
              plt.xticks(rotation=15, ha='right')
              plt.tight_layout()
              safe_save_plot(fig, os.path.join(PLOT_DIR, 'bar_resilience_scc_impact.png'), "Resilience SCC Impact Bar Plot")
         else: logging.warning("No valid data for plotting resilience SCC impact.")
    else: logging.warning("Resilience summary data/columns insufficient for plotting SCC impact.")

    # --- Generate Spatial Visualizations ---
    logging.info("\n--- Generating Spatial Visualizations ---")
    if not GEOPANDAS_AVAILABLE or not CONTEXTILY_AVAILABLE:
        logging.warning("GeoPandas/Contextily not available. Skipping spatial plots.")
    elif stops_df is None:
         logging.warning("Stops data not loaded. Skipping spatial plots.")
    else:
        stops_geo: Optional[GeoDataFrame] = None
        try:
            stops_df['stop_lon'] = pd.to_numeric(stops_df['stop_lon'], errors='coerce')
            stops_df['stop_lat'] = pd.to_numeric(stops_df['stop_lat'], errors='coerce')
            stops_df_clean = stops_df.dropna(subset=['stop_lon', 'stop_lat', 'stop_id']).copy() # Ensure stop_id not NaN
            stops_df_clean['stop_id'] = stops_df_clean['stop_id'].astype(str) # Ensure index type later
            stops_df_clean = stops_df_clean.drop_duplicates(subset=['stop_id']) # Ensure unique index

            if not stops_df_clean.empty:
                stops_geo = gpd.GeoDataFrame(
                    stops_df_clean,
                    geometry=gpd.points_from_xy(stops_df_clean.stop_lon, stops_df_clean.stop_lat),
                    crs="EPSG:4326"
                )
                logging.info(f"Created stops GeoDataFrame with {len(stops_geo)} unique stops.")
                stops_geo = stops_geo.to_crs(epsg=3857) # Project for basemaps
                logging.info("Reprojected stops GeoDataFrame to EPSG:3857.")
                stops_geo.set_index('stop_id', inplace=True, verify_integrity=True) # Set index *after* projection
            else: logging.warning("No valid coordinates/IDs in stops data.")
        except Exception as e: logging.error(f"Error creating/projecting stops GeoDataFrame: {e}", exc_info=True)

        if stops_geo is not None and not stops_geo.empty:
            # Plot Accessibility
            if accessibility_df is not None:
                access_cols = [col for col in accessibility_df.columns if 'reachable_targets_' in col]
                if access_cols:
                    try:
                         access_cols_sorted = sorted(access_cols, key=lambda x: int(x.split('_')[-1].replace('min','')), reverse=True)
                         col_to_plot = access_cols_sorted[0]
                         time_val = col_to_plot.split('_')[-1]
                         # Ensure stop_id in accessibility_df is string for merge
                         accessibility_df['stop_id'] = accessibility_df['stop_id'].astype(str)
                         access_merged = stops_geo.merge(accessibility_df[['stop_id', col_to_plot]], left_index=True, right_on='stop_id', how='inner')
                         plot_spatial_distribution(access_merged, col_to_plot, f'Accessibility: Reachable Targets ({time_val})', f'Reachable Targets ({time_val})', f'map_accessibility_{time_val}.png', cmap='viridis')
                    except Exception as e: logging.error(f"Failed to plot accessibility map: {e}")
                else: logging.warning("No accessibility columns for spatial plot.")
            else: logging.warning("Accessibility data not loaded, skipping spatial plot.")

            # Plot Betweenness
            if betweenness_df is not None:
                try:
                    betweenness_df['stop_id'] = betweenness_df['stop_id'].astype(str)
                    betweenness_merged = stops_geo.merge(betweenness_df[['stop_id', 'betweenness_centrality']], left_index=True, right_on='stop_id', how='inner')
                    plot_spatial_distribution(betweenness_merged, 'betweenness_centrality', 'Spatial Distribution of Betweenness Centrality', 'Betweenness', 'map_betweenness.png', cmap='plasma', log_scale=True, markersize=20)
                except Exception as e: logging.error(f"Failed to plot betweenness map: {e}")
            else: logging.warning("Betweenness data not loaded, skipping spatial plot.")

            # Plot Degree
            if degree_df is not None:
                try:
                    degree_df['stop_id'] = degree_df['stop_id'].astype(str)
                    degree_merged = stops_geo.merge(degree_df[['stop_id', 'total_degree']], left_index=True, right_on='stop_id', how='inner')
                    plot_spatial_distribution(degree_merged, 'total_degree', 'Spatial Distribution of Stop Degree', 'Total Degree', 'map_degree.png', cmap='coolwarm', markersize=15)
                except Exception as e: logging.error(f"Failed to plot degree map: {e}")
            else: logging.warning("Degree data not loaded, skipping spatial plot.")

            # Plot Circuitous Routes
            if circuity_df is not None and route_sequences is not None:
                try:
                    circuity_df['circuity_index'] = pd.to_numeric(circuity_df['circuity_index'], errors='coerce')
                    top_circuity_routes = circuity_df.dropna(subset=['circuity_index']).nlargest(TOP_N_CIRCUITY_PLOT, 'circuity_index').copy()
                    if not top_circuity_routes.empty:
                        # --- Safely merge route names ---
                        final_route_name_col = 'route_id' # Default
                        if routes_df is not None:
                            name_cols_pref = ['route_short_name', 'route_long_name', 'route_id']
                            available_name_col = next((col for col in name_cols_pref if col in routes_df.columns), None)
                            if available_name_col and available_name_col != 'route_id':
                                try:
                                    top_circuity_routes['route_id'] = top_circuity_routes['route_id'].astype(str)
                                    routes_df['route_id'] = routes_df['route_id'].astype(str)
                                    top_circuity_routes = pd.merge(
                                        top_circuity_routes, routes_df[['route_id', available_name_col]],
                                        on='route_id', how='left'
                                    )
                                    if available_name_col in top_circuity_routes.columns:
                                         top_circuity_routes[available_name_col].fillna('', inplace=True)
                                         final_route_name_col = available_name_col
                                    else: logging.warning(f"Merge failed to add '{available_name_col}'.")
                                except Exception as merge_e: logging.warning(f"Merge failed: {merge_e}")
                            else: logging.info("Using route_id for display names.")

                        top_circuity_routes['display_name'] = top_circuity_routes[final_route_name_col].astype(str)
                        top_circuity_routes.loc[top_circuity_routes['display_name'] == '', 'display_name'] = top_circuity_routes['route_id']
                        top_circuity_routes['route_id'] = top_circuity_routes['route_id'].astype(str) # Ensure key is string

                        plot_circuitous_routes_map(top_circuity_routes, route_sequences, stops_geo, 'map_top_circuitous_routes.png', TOP_N_CIRCUITY_PLOT)
                    else: logging.warning("No valid circuity data found.")
                except Exception as e: logging.error(f"Failed to prepare/plot circuitous routes: {e}")
            else: logging.warning("Circuity data or sequences missing, skipping route map.")
        else: logging.warning("Stops GeoDataFrame invalid. Skipping all spatial plots.")

    # --- Script Finish ---
    total_runtime = time.time() - main_start_time
    logging.info("=" * 50)
    logging.info(f"Analysis Visualization Script Finished")
    logging.info(f"Total execution time: {total_runtime:.2f} seconds")
    logging.info(f"Plots saved in: {os.path.abspath(PLOT_DIR)}")
    logging.info("=" * 50)


if __name__ == "__main__":
    run_visualization()